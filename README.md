# ğŸŒŒ LLM Lightwave: The Future of Photonic-Inspired Intelligence

![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)
![Python: 3.8+](https://img.shields.io/badge/Python-3.8%2B-brightgreen.svg)
![PyTorch: 2.0+](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c.svg)

> **"Where Light Meets Language."**  
> LLM Lightwave is a revolutionary, complex-valued language model inspired by the principles of photonic interference and quantum wave propagation. By operating in the complex domain ($\mathbb{C}$), it captures the phase and magnitude of linguistic features with unprecedented efficiency.

---

## ğŸš€ Why Lightwave?

Traditional Large Language Models (LLMs) treat tokens as static points in a real-valued vector space. **Lightwave evolves them.**

Built on the cutting edge of **Complex-Valued Neural Networks (CVNNs)**, Lightwave simulates the behavior of lightwaves as they cascade through interference layers, allowing for:
- **Phase-Aware Context**: Naturally encoding sequence timing and relational depth.
- **Unitary Efficiency**: Extremely low-parameter "Wave" mode using Quantum Random Walks.
- **Triple-Adaptive Interference**: Coupling that learns unique dynamics across **Epochs**, **Steps (M)**, and **Layers** simultaneously.
- **Dynamic Evolution**: A stateful, temporal architecture that treats memory as a field.

---

## âœ¨ Key Features

### ğŸ§  Complex-Valued Core
*   **16-Dimensional $\mathbb{C}$ Space**: High-fidelity embeddings for rich feature representation.
*   **Mish-Complex Activation**: Non-linear mapping preserving phase information across deep cascades.
*   **Amplitude & Phase Modulation**: Dual-domain learning that tunes field intensity and rotation for precise interference.
*   **Complex LayerNorm**: Independent normalization of magnitude and phase for maximum stability.

### âš¡ Advanced Architectures
*   **Multi-Head Modulator**: A Dot-Product Attention upgrade designed for the complex domain.
*   **Quantum Wave Mode**: Run with **zero trainable parameters** in the modulator, utilizing pure unitary interference.
*   **Cascaded Interference**: Scalable multi-stage propagation (up to 12+ layers) for deep reasoning.

### ğŸ› ï¸ Professional Tooling
*   **RLVR (Reinforcement Learning from Verifiable Rewards)**: Self-improving policy gradient loop for targeted task success.
*   **PEFT (LoRA & 4-bit Quantization)**: Fine-tune massive architectures on consumer hardware and prepare for optical substrates.
*   **Beam Search Decoding**: High-precision text generation with configurable width.

---

## ğŸ“¦ Installation

```bash
# Clone the repository
git clone https://github.com/andrelmm91/LLM-Lightwave
cd LLM-Lightwave

# Install dependencies
pip install -r requirements.txt
```

---

## ğŸ® Quick Start

### 1. Training from Scratch
Fire up a standard neural training run on the TinyStories dataset:
```bash
python llm_light.py --train --mode neural --layers 4 --epochs 15
```

### 2. Deep Wave Interference
Experience the efficiency of unitary propagation:
```bash
python llm_light.py --train --mode wave --layers 12
```

### 3. Generate with Beam Search
Generate high-quality stories with an existing checkpoint:
```bash
python llm_light.py --generate --load --beam --beam_width 5 --prompt "A long time ago in a photonic city..."
```

---

## ğŸ”¬ Technical Documentation

For a deep dive into the mathematics, architecture, and CLI options, please refer to our comprehensive technical guide:

ğŸ‘‰ **[Read the Full Documentation (doc.md)](documentation/doc.md)**

---

## ğŸŒŸ License

LLM Lightwave is released under the **MIT License**. Feel free to use, modify, and distribute for research or production.

---

## ğŸ¤ Contributing

We welcome contributions from researchers in CVNNs, Photonics, and NLP. Open an issue or submit a PR!

---

*â€œThe next leap in AI isn't just biggerâ€”it's faster, holographic, and complex.â€* ğŸŒŠğŸ›°ï¸
