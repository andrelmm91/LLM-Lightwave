# üåå LLM Lightwave: The Future of Photonic-Inspired Intelligence

![License: MIT](https://img.shields.io/badge/License-MIT-blue.svg)
![Python: 3.8+](https://img.shields.io/badge/Python-3.8%2B-brightgreen.svg)
![PyTorch: 2.0+](https://img.shields.io/badge/PyTorch-2.0%2B-ee4c2c.svg)

> **"Where Light Meets Language."**  
> LLM Lightwave is a revolutionary, complex-valued language model inspired by the principles of photonic interference and quantum wave propagation. By operating in the complex domain ($\mathbb{C}$), it captures the phase and magnitude of linguistic features with unprecedented efficiency.

---

## üöÄ Why Lightwave?

Traditional Large Language Models (LLMs) treat tokens as static points in a real-valued vector space. **Lightwave evolves them.**

Built on the cutting edge of **Complex-Valued Neural Networks (CVNNs)**, Lightwave simulates the behavior of lightwaves as they cascade through interference layers, allowing for:
- **Phase-Aware Context**: Naturally encoding sequence timing and relational depth.
- **Unitary Efficiency**: Extremely low-parameter "Wave" mode using Quantum Random Walks.
*   **Triple-Adaptive Interference**: Coupling and Phase that learn unique dynamics across **Epochs**, **Steps (M)**, and **Layers** simultaneously.
- **Dynamic Evolution**: A stateful, temporal architecture that treats memory as a field.

---

## ‚ú® Key Features

### üß† Complex-Valued Core
*   **High-Dimensional $\mathbb{C}$ Space**: Scaled-up embeddings for enhanced linguistic capacity and reduced feature collision.
*   **Mish-Complex Activation**: Non-linear mapping preserving phase information across deep cascades.
*   **Amplitude & Phase Modulation**: Dual-domain learning that tunes field intensity and rotation for precise interference.
*   **Complex LayerNorm**: Independent normalization of magnitude and phase for maximum stability.

### ‚ö° Advanced Architectures
*   **Multi-Head Modulator**: A Dot-Product Attention upgrade designed for the complex domain.
*   **Quantum Wave Mode**: Run with **zero trainable parameters** in the modulator core, utilizing pure unitary interference with **Ballistic Spread** and **Reflecting Boundaries** for enhanced feature mixing.
*   **Cascaded Interference**: Scalable multi-stage propagation (up to 12+ layers) for deep reasoning.

### üõ†Ô∏è Professional Tooling
*   **RLVR (Reinforcement Learning from Verifiable Rewards)**: Self-improving policy gradient loop for targeted task success.
*   **PEFT (LoRA & 4-bit Quantization)**: Fine-tune massive architectures on consumer hardware and prepare for optical substrates.
*   **Beam Search Decoding**: High-precision text generation with configurable width.
*   **Mini-Batch Training**: Parallel processing of multiple sequences for 10-20x GPU utilization improvement.

---

## üì¶ Installation

```bash
# Clone the repository
git clone https://github.com/andrelmm91/LLM-Lightwave
cd LLM-Lightwave

# Install dependencies
pip install -r requirements.txt
```

---

## üéÆ Quick Start

### 1. Training from Scratch
Fire up a standard neural training run on the TinyStories dataset:
```bash
python llm_light.py --train --mode neural --layers 4 --epochs 15
```

### 2. Deep Wave Interference
Experience the efficiency of unitary propagation:
```bash
python llm_light.py --train --mode wave --layers 12
```

### 3. High-Performance Training
Leverage optimized batching and scaled capacity:
```bash
python llm_light.py --train --mode wave --batch_size 16 --steps 200 --M 32 --lora
```

### 4. Generate with Beam Search
Generate high-quality stories with an existing checkpoint:
```bash
python llm_light.py --generate --load --beam --beam_width 5 --prompt "A long time ago in a photonic city..."
```

---

## üî¨ Technical Documentation

For a deep dive into the mathematics, architecture, and CLI options, please refer to our comprehensive technical guide:

üëâ **[Read the Full Documentation (doc.md)](documentation/doc.md)**

---

## üåü License

LLM Lightwave is released under the **MIT License**. Feel free to use, modify, and distribute for research or production.

---

## ü§ù Contributing

We welcome contributions from researchers in CVNNs, Photonics, and NLP. Open an issue or submit a PR!

---

*‚ÄúThe next leap in AI isn't just bigger‚Äîit's faster, holographic, and complex.‚Äù* üåäüõ∞Ô∏è
